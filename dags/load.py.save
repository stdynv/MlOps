import os
import json
from urllib.parse import urlparse, parse_qs
import time
import requests
from datetime import datetime, timedelta
from airflow.models.dag import DAG
from airflow.operators.python import PythonOperator

import logging
logger = logging.getLogger(__name__)

# credentials
DATA_PATH = f"/opt/airflow/data/"
URL_FILE = os.path.join(DATA_PATH, "api", "url.json")
RESULTS_FILE = os.path.join(DATA_PATH, "api", "results.json")

# rename columns : 
def rename_columns(columns: t.List[str]) -> t.List[str]:

    """
    rename columns
    """
    columns = [col.lower() for col in columns]


    rgxs = [

        (r"[°|/|']", "_"),

        (r"²", "2"),

        (r"[(|)]", ""),

        (r"é|è", "e"),

        (r"â", "a"),

        (r"^_", "dpe_"),

        (r"_+", "_"),

    ]

    for rgx in rgxs:

        columns = [re.sub(rgx[0], rgx[1], col) for col in columns]
    return columns
# check environment setup 
def check_environment_setup():

    logger.info("--" * 20)
    logger.info(f"[info logger] cwd: {os.getcwd()}")
    assert os.path.isfile(URL_FILE)
    logger.info(f"[info logger] URL_FILE: {URL_FILE}")
    logger.info("--" * 20)


def interrogate_api():
    """
    Interrogates the ADEME API using the specified URL and payload from a JSON file.

    - Reads the URL and payload from a JSON file defined by the constant `URL_FILE`.
    - Performs a GET request to the obtained URL with the given payload.
    - Saves the results to a JSON file defined by the constant `RESULTS_FILE`.

    Raises:
        AssertionError: If the URL file does not exist, or if the retrieved URL or payload is None.
        requests.exceptions.RequestException: If the GET request encounters an error.

    """
    # test url file exists
    #assert os.path.isfile(URL_FILE)
    # open url file
    with open(URL_FILE, encoding="utf-8") as file:
        url = json.load(file)
    assert url.get("url") is not None
    assert url.get("payload") is not None

    # make GET requests
    results = requests.get(url.get("url"), params=url.get("payload"), timeout=5)
    assert results.raise_for_status() is None

    data = results.json()

    # save results to file
    with open(RESULTS_FILE, "w", encoding="utf-8") as file:
        json.dump(data, file, indent=4, ensure_ascii=False)

def process_results():
    # test url file exists
    assert os.path.isfile(RESULTS_FILE)

    # read previous API call output
    with open(RESULTS_FILE, encoding="utf-8") as file:
        data = json.load(file)

    # new url is same as old url
    base_url = data.get("next").split("?")[0]

    # extract payload as dict
    parsed_url = urlparse(data.get("next"))
    query_params = parse_qs(parsed_url.query)
    new_payload = {k: v[0] if len(v) == 1 else v for k, v in query_params.items()}

    # save new url (same as old url) with new payload into url.json
    new_url = {"url": base_url, "payload": new_payload}

    with open(URL_FILE, "w", encoding="utf-8") as file:
        json.dump(new_url, file, indent=4, ensure_ascii=False)

# save records to database : 
def save_postgresdb():
    assert os.path.isfile(RESULTS_FILE)

    # read previous API call output
    with open(RESULTS_FILE, encoding="utf-8") as file:
        data = json.load(file)

    data = pd.DataFrame(data["results"])
    
    # set columns
    new_columns = rename_columns(data.columns)
    data.columns = new_columns
    data = data.astype(str).replace("nan", "")

    # now check that the data does not have columns not already in the table
    db = Database()

    # store data into table 
    data[["n_dpe", "payload"]].to_sql(
        name="dpe_logement", con=db.engine, if_exists="append", index=False
       

    logger.info(f"loaded {data.shape}")

    # to_sql
    data.to_sql(name="dpe_tertiaire", con=db.engine, if_exists="append", index=False)
    db.close()







default_args = {
    "depends_on_past": False,
    "email": ["airflow@example.com"],
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

with DAG(
    "live_ademe",
    default_args=default_args,
    description="Get ademe data",
    schedule=None,
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=["example"],
) as dag:


    check_environment_setup_task = PythonOperator(
        task_id="check_environment_setup",
        python_callable=check_environment_setup,
    )

    interrogate_api = PythonOperator(
        task_id="interrogate_api",
        python_callable=interrogate_api,
    )

    check_environment_setup_task >> interrogate_api

